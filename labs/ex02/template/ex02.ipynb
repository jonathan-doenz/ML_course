{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "height, weight, gender = load_data(sub_sample=True, add_outlier=True)\n",
    "# height, weight, gender = load_data(sub_sample=True, add_outlier=False)\n",
    "# height, weight, gender = load_data(sub_sample=False, add_outlier=False)\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((202,), (202, 2))"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Computing the Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the `compute_loss` function below:\n",
    "<a id='compute_loss'></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_MSE(y, tx, w):\n",
    "    \"\"\"Calculate the loss using mean squared error (MSE).\"\"\"\n",
    "    N = len(y)\n",
    "    e = y - tx @ w\n",
    "    return e.T @ e / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_MAE(y, tx, w):\n",
    "    \"\"\"Calculate the loss using mean absolute error (MAE).\"\"\"\n",
    "    N = len(y)\n",
    "    e = y - tx @ w\n",
    "    return np.abs(e).T @ np.ones(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(y, tx, w, method=\"MSE\"):\n",
    "    \"\"\"Calculate the loss using mean squared error (MSE).\"\"\"\n",
    "    N = len(y)\n",
    "    e = y - tx @ w\n",
    "    if method == \"MSE\":\n",
    "        loss = e.T @ e / N\n",
    "    elif method == \"MAE\":\n",
    "        loss = np.abs(e).T @ np.ones(N)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14759.69678269513"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_loss_2(y, tx, w, \"MAE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5552.395037897896"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_loss(y, tx, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Grid Search\n",
    "\n",
    "Fill in the function `grid_search()` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "element 0 of w equals 1\n",
      "element 1 of w equals 2\n"
     ]
    }
   ],
   "source": [
    "for i, el in enumerate(w):\n",
    "    # print(\"hello \" + str(i))\n",
    "    print(\"element {} of w equals {}\".format(i, el))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(y, tx, w0, w1, method=\"MSE\"):\n",
    "    \"\"\"Algorithm for grid search.\"\"\"\n",
    "    losses = np.zeros((len(w0), len(w1)))\n",
    "    for i, w_0 in enumerate(w0):\n",
    "        for j, w_1 in enumerate(w1):\n",
    "            losses[i, j] = compute_loss(y, tx, [w_0, w_1], method)\n",
    "            # losses[i, j] = compute_loss(y, tx, [w_0, w_1])\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us play with the grid search demo now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search: loss*=142.25403922169858, w0*=71.42857142857142, w1*=9.183673469387742, execution time=0.053 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from grid_search import generate_w, get_best_parameters\n",
    "from plots import grid_visualization\n",
    "\n",
    "# Generate the grid of parameters to be swept\n",
    "grid_w0, grid_w1 = generate_w(num_intervals=50)\n",
    "\n",
    "# Start the grid search\n",
    "start_time = datetime.datetime.now()\n",
    "grid_losses = grid_search(y, tx, grid_w0, grid_w1)\n",
    "\n",
    "# Select the best combinaison\n",
    "loss_star, w0_star, w1_star = get_best_parameters(grid_w0, grid_w1, grid_losses)\n",
    "end_time = datetime.datetime.now()\n",
    "execution_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "# Print the results\n",
    "print(\"Grid Search: loss*={l}, w0*={w0}, w1*={w1}, execution time={t:.3f} seconds\".format(\n",
    "      l=loss_star, w0=w0_star, w1=w1_star, t=execution_time))\n",
    "\n",
    "# Plot the results\n",
    "fig = grid_visualization(grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight)\n",
    "fig.set_size_inches(10.0,6.0)\n",
    "fig.savefig(\"grid_plot\")  # Optional saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, please fill in the functions `compute_gradient` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_error_vec(y, tx, w):\n",
    "    \"\"\"Compute the error vector: y - Xw\"\"\"\n",
    "    return y - tx @ w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Compute the gradient.\"\"\"\n",
    "    N = len(y)\n",
    "    e = y - tx @ w\n",
    "    return - tx.T @ e / N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill in the functions `gradient_descent` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"Gradient descent algorithm.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    N = len(y)\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: compute gradient and loss\n",
    "        # ***************************************************\n",
    "        gradient = compute_gradient(y, tx, w)\n",
    "        e = compute_error_vec(y, tx, w)\n",
    "        loss = e.T @ e / (2 * N)\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: update w by gradient\n",
    "        # ***************************************************\n",
    "        # w -= gamma * gradient\n",
    "        w = w - gamma * gradient\n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subgradient Descent(0/49): loss=14961.69678269513, w0=0.1, w1=8.727891896558162e-17\n",
      "Subgradient Descent(1/49): loss=14941.496782695129, w0=0.2, w1=1.7455783793116323e-16\n",
      "Subgradient Descent(2/49): loss=14921.29678269513, w0=0.30000000000000004, w1=2.6183675689674487e-16\n",
      "Subgradient Descent(3/49): loss=14901.096782695127, w0=0.4, w1=3.4911567586232647e-16\n",
      "Subgradient Descent(4/49): loss=14880.89678269513, w0=0.5, w1=4.3639459482790806e-16\n",
      "Subgradient Descent(5/49): loss=14860.69678269513, w0=0.6, w1=5.236735137934897e-16\n",
      "Subgradient Descent(6/49): loss=14840.496782695129, w0=0.7, w1=6.109524327590712e-16\n",
      "Subgradient Descent(7/49): loss=14820.296782695132, w0=0.7999999999999999, w1=6.982313517246528e-16\n",
      "Subgradient Descent(8/49): loss=14800.096782695127, w0=0.8999999999999999, w1=7.855102706902344e-16\n",
      "Subgradient Descent(9/49): loss=14779.89678269513, w0=0.9999999999999999, w1=8.72789189655816e-16\n",
      "Subgradient Descent(10/49): loss=14759.69678269513, w0=1.0999999999999999, w1=9.600681086213977e-16\n",
      "Subgradient Descent(11/49): loss=14739.496782695129, w0=1.2, w1=1.0473470275869793e-15\n",
      "Subgradient Descent(12/49): loss=14719.296782695132, w0=1.3, w1=1.1346259465525609e-15\n",
      "Subgradient Descent(13/49): loss=14699.096782695127, w0=1.4000000000000001, w1=1.2219048655181425e-15\n",
      "Subgradient Descent(14/49): loss=14678.89678269513, w0=1.5000000000000002, w1=1.309183784483724e-15\n",
      "Subgradient Descent(15/49): loss=14658.69678269513, w0=1.6000000000000003, w1=1.3964627034493057e-15\n",
      "Subgradient Descent(16/49): loss=14638.496782695127, w0=1.7000000000000004, w1=1.4837416224148873e-15\n",
      "Subgradient Descent(17/49): loss=14618.296782695132, w0=1.8000000000000005, w1=1.5710205413804689e-15\n",
      "Subgradient Descent(18/49): loss=14598.096782695127, w0=1.9000000000000006, w1=1.6582994603460504e-15\n",
      "Subgradient Descent(19/49): loss=14577.89678269513, w0=2.0000000000000004, w1=1.745578379311632e-15\n",
      "Subgradient Descent(20/49): loss=14557.69678269513, w0=2.1000000000000005, w1=1.832857298277214e-15\n",
      "Subgradient Descent(21/49): loss=14537.496782695127, w0=2.2000000000000006, w1=1.9201362172427954e-15\n",
      "Subgradient Descent(22/49): loss=14517.296782695132, w0=2.3000000000000007, w1=2.007415136208377e-15\n",
      "Subgradient Descent(23/49): loss=14497.096782695127, w0=2.400000000000001, w1=2.0946940551739586e-15\n",
      "Subgradient Descent(24/49): loss=14476.89678269513, w0=2.500000000000001, w1=2.18197297413954e-15\n",
      "Subgradient Descent(25/49): loss=14456.69678269513, w0=2.600000000000001, w1=2.2692518931051218e-15\n",
      "Subgradient Descent(26/49): loss=14436.496782695127, w0=2.700000000000001, w1=2.3565308120707034e-15\n",
      "Subgradient Descent(27/49): loss=14416.296782695132, w0=2.800000000000001, w1=2.443809731036285e-15\n",
      "Subgradient Descent(28/49): loss=14396.096782695127, w0=2.9000000000000012, w1=2.5310886500018666e-15\n",
      "Subgradient Descent(29/49): loss=14375.896782695128, w0=3.0000000000000013, w1=2.618367568967448e-15\n",
      "Subgradient Descent(30/49): loss=14355.69678269513, w0=3.1000000000000014, w1=2.7056464879330297e-15\n",
      "Subgradient Descent(31/49): loss=14335.496782695125, w0=3.2000000000000015, w1=2.7929254068986113e-15\n",
      "Subgradient Descent(32/49): loss=14315.296782695132, w0=3.3000000000000016, w1=2.880204325864193e-15\n",
      "Subgradient Descent(33/49): loss=14295.096782695127, w0=3.4000000000000017, w1=2.9674832448297745e-15\n",
      "Subgradient Descent(34/49): loss=14274.896782695128, w0=3.5000000000000018, w1=3.054762163795356e-15\n",
      "Subgradient Descent(35/49): loss=14254.69678269513, w0=3.600000000000002, w1=3.1420410827609377e-15\n",
      "Subgradient Descent(36/49): loss=14234.496782695127, w0=3.700000000000002, w1=3.2293200017265193e-15\n",
      "Subgradient Descent(37/49): loss=14214.296782695132, w0=3.800000000000002, w1=3.316598920692101e-15\n",
      "Subgradient Descent(38/49): loss=14194.096782695127, w0=3.900000000000002, w1=3.4038778396576825e-15\n",
      "Subgradient Descent(39/49): loss=14173.896782695128, w0=4.000000000000002, w1=3.491156758623264e-15\n",
      "Subgradient Descent(40/49): loss=14153.69678269513, w0=4.100000000000001, w1=3.578435677588846e-15\n",
      "Subgradient Descent(41/49): loss=14133.496782695127, w0=4.200000000000001, w1=3.665714596554428e-15\n",
      "Subgradient Descent(42/49): loss=14113.296782695132, w0=4.300000000000001, w1=3.75299351552001e-15\n",
      "Subgradient Descent(43/49): loss=14093.096782695127, w0=4.4, w1=3.840272434485592e-15\n",
      "Subgradient Descent(44/49): loss=14072.896782695128, w0=4.5, w1=3.927551353451174e-15\n",
      "Subgradient Descent(45/49): loss=14052.69678269513, w0=4.6, w1=4.0148302724167556e-15\n",
      "Subgradient Descent(46/49): loss=14032.496782695127, w0=4.699999999999999, w1=4.1021091913823376e-15\n",
      "Subgradient Descent(47/49): loss=14012.296782695128, w0=4.799999999999999, w1=4.1893881103479196e-15\n",
      "Subgradient Descent(48/49): loss=13992.09678269513, w0=4.899999999999999, w1=4.2766670293135016e-15\n",
      "Subgradient Descent(49/49): loss=13971.896782695128, w0=4.999999999999998, w1=4.3639459482790835e-15\n",
      "Subgradient Descent: execution time=0.021 seconds\n"
     ]
    }
   ],
   "source": [
    "# from gradient_descent import *\n",
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "# max_iters = 100\n",
    "# gamma = 0.7\n",
    "# gamma = 0.001\n",
    "gamma = 0.1\n",
    "# gamma = 0.1\n",
    "# gamma = 0.5\n",
    "# gamma = 1\n",
    "# gamma = 2\n",
    "# gamma = 2.5\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "# w_initial = np.array([100, 10])\n",
    "# w_initial = np.array([-1000, 1000])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "subgradient_losses, subgradient_ws = subgradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Subgradient Descent: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your gradient descent function through gradient descent demo shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8818add03ef54da49d597ca2ff21ff9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widgeâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gradient_losses, gradient_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gradient_ws)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(y)\n",
    "e = compute_error_vec(y, tx, [73.29392200210519, 13.479712434989048])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69.21874468427745"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.T @ e / (2*N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[108.82253199]\n",
      "[[1.         2.20008905]]\n"
     ]
    }
   ],
   "source": [
    "for minibatch_y, minibatch_tx in batch_iter(y, tx, 1):\n",
    "    print(minibatch_y)\n",
    "    print(minibatch_tx)\n",
    "    # print(minibatch_y.shape)\n",
    "    # print(minibatch_tx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stoch_gradient(y, tx, w):\n",
    "    \"\"\"Compute a stochastic gradient from just few examples n and their corresponding y_n labels.\"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: implement stochastic gradient computation.It's same as the gradient descent.\n",
    "    # ***************************************************\n",
    "    return compute_gradient(y, tx, w)\n",
    "\n",
    "    # for minibatch_y, minibatch_tx in batch_iter(y, tx, batch_size=1):\n",
    "    #     return compute_gradient(minibatch_y, minibatch_tx, w)\n",
    "\n",
    "def stochastic_gradient_descent(\n",
    "        y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"Stochastic gradient descent algorithm.\"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: implement stochastic gradient descent.\n",
    "    # ***************************************************\n",
    "    # Define parameters to store w and loss\n",
    "    N = len(y)\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: compute gradient and loss\n",
    "        # ***************************************************\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, batch_size):\n",
    "        # stoch_gradient = compute_stoch_gradient(minibatch_y, minibatch_tx, w) for minibatch_y, minibatch_tx in batch_iter(y, tx, batch_size)\n",
    "            stoch_gradient = compute_stoch_gradient(minibatch_y, minibatch_tx, w)\n",
    "            e = compute_error_vec(minibatch_y, minibatch_tx, w)\n",
    "            loss = e.T @ e / (2 * N)\n",
    "            # ***************************************************\n",
    "            # INSERT YOUR CODE HERE\n",
    "            # TODO: update w by gradient\n",
    "            # ***************************************************\n",
    "            # w -= gamma * gradient\n",
    "            w = w - gamma * stoch_gradient\n",
    "            # store w and loss\n",
    "            ws.append(w)\n",
    "            losses.append(loss)\n",
    "            print(\"Stochastic Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                  bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(0/49): loss=15.12022945161985, w0=7.815735856881564, w1=2.260276086070039\n",
      "Stochastic Gradient Descent(1/49): loss=14.254888041556352, w0=15.404526780861989, w1=7.140733189170231\n",
      "Stochastic Gradient Descent(2/49): loss=4.96023026808361, w0=19.881057835490985, w1=5.404605381511164\n",
      "Stochastic Gradient Descent(3/49): loss=5.0298832165075, w0=24.38890967363854, w1=4.233440722149872\n",
      "Stochastic Gradient Descent(4/49): loss=8.323169532282815, w0=30.187668584855885, w1=5.176321628424395\n",
      "Stochastic Gradient Descent(5/49): loss=7.168104811462566, w0=35.56903861843551, w1=6.685754359580085\n",
      "Stochastic Gradient Descent(6/49): loss=2.746988010496552, w0=38.90037942393802, w1=1.7985241676881314\n",
      "Stochastic Gradient Descent(7/49): loss=6.382814161044149, w0=43.97842719947326, w1=9.462032629225643\n",
      "Stochastic Gradient Descent(8/49): loss=1.3105483742326551, w0=46.27943033547719, w1=6.251155397388372\n",
      "Stochastic Gradient Descent(9/49): loss=2.645308349239677, w0=49.54853506474609, w1=8.018992820228252\n",
      "Stochastic Gradient Descent(10/49): loss=2.448355918036834, w0=52.69358799098896, w1=9.939798334345198\n",
      "Stochastic Gradient Descent(11/49): loss=1.1059710558775264, w0=54.807381515850174, w1=9.385785797501777\n",
      "Stochastic Gradient Descent(12/49): loss=1.9424332379670157, w0=57.60870803758711, w1=10.171536250672446\n",
      "Stochastic Gradient Descent(13/49): loss=0.16525951781347806, w0=58.42580561558282, w1=9.99591084221316\n",
      "Stochastic Gradient Descent(14/49): loss=0.35628953210288256, w0=59.625559636078535, w1=8.83326864833937\n",
      "Stochastic Gradient Descent(15/49): loss=0.1623753183954925, w0=60.43549561275681, w1=8.427910810776229\n",
      "Stochastic Gradient Descent(16/49): loss=0.8271805864308502, w0=62.263557310069196, w1=11.479026219656658\n",
      "Stochastic Gradient Descent(17/49): loss=0.6007269871147384, w0=63.82142027835092, w1=11.916385304634213\n",
      "Stochastic Gradient Descent(18/49): loss=0.6411641610428578, w0=65.43086217249098, w1=11.03780020695589\n",
      "Stochastic Gradient Descent(19/49): loss=0.2024779674867422, w0=66.33530177171606, w1=11.853301531455477\n",
      "Stochastic Gradient Descent(20/49): loss=0.3986782251267503, w0=67.60441958717374, w1=11.218855155532507\n",
      "Stochastic Gradient Descent(21/49): loss=0.282889605441068, w0=68.67347244179876, w1=11.259131951867499\n",
      "Stochastic Gradient Descent(22/49): loss=0.12668932289688725, w0=69.38889180448294, w1=11.798819601859353\n",
      "Stochastic Gradient Descent(23/49): loss=0.00272317154196681, w0=69.28400323013118, w1=11.735355832985073\n",
      "Stochastic Gradient Descent(24/49): loss=0.1118040743909014, w0=69.95608094925244, w1=12.098796053263715\n",
      "Stochastic Gradient Descent(25/49): loss=0.004991270921145663, w0=69.81407836293293, w1=12.18517138774129\n",
      "Stochastic Gradient Descent(26/49): loss=0.18479931443329325, w0=70.67813231397447, w1=12.712882568626826\n",
      "Stochastic Gradient Descent(27/49): loss=0.25626018985131377, w0=71.69562490215353, w1=14.9514568752592\n",
      "Stochastic Gradient Descent(28/49): loss=0.1786920675986656, w0=70.84596852861266, w1=15.280978113704245\n",
      "Stochastic Gradient Descent(29/49): loss=0.22745161111405407, w0=69.8873734582619, w1=15.203866514567421\n",
      "Stochastic Gradient Descent(30/49): loss=0.017170447775779706, w0=70.15075266969316, w1=15.520369376239895\n",
      "Stochastic Gradient Descent(31/49): loss=0.11666328921051324, w0=70.8372799367485, w1=16.238246021899993\n",
      "Stochastic Gradient Descent(32/49): loss=0.19241842697288156, w0=69.9555937992991, w1=15.475709498762363\n",
      "Stochastic Gradient Descent(33/49): loss=0.0038896230907588068, w0=69.83023799653486, w1=15.56614793188095\n",
      "Stochastic Gradient Descent(34/49): loss=0.003566008971064664, w0=69.7102101820788, w1=15.638720079233936\n",
      "Stochastic Gradient Descent(35/49): loss=0.3192349693917201, w0=70.84586386007335, w1=15.957262164637058\n",
      "Stochastic Gradient Descent(36/49): loss=0.04040769925873212, w0=71.24990235453308, w1=16.0083600550205\n",
      "Stochastic Gradient Descent(37/49): loss=0.00016496532658493994, w0=71.2757182395538, w1=16.013240124937482\n",
      "Stochastic Gradient Descent(38/49): loss=0.016861749689238176, w0=71.53671913753027, w1=15.975850963582953\n",
      "Stochastic Gradient Descent(39/49): loss=0.033767302257148635, w0=71.16736848961033, w1=16.20811725292196\n",
      "Stochastic Gradient Descent(40/49): loss=0.05557355779471165, w0=71.64120092147905, w1=15.686344441585387\n",
      "Stochastic Gradient Descent(41/49): loss=0.0017988244995743377, w0=71.55595274860549, w1=15.65409808906733\n",
      "Stochastic Gradient Descent(42/49): loss=0.2279931596820475, w0=72.51568831901824, w1=16.035424296281693\n",
      "Stochastic Gradient Descent(43/49): loss=0.016655302920298654, w0=72.77508651445916, w1=16.135529191316476\n",
      "Stochastic Gradient Descent(44/49): loss=0.1645053153259813, w0=73.5903174473346, w1=16.36419517151376\n",
      "Stochastic Gradient Descent(45/49): loss=0.131214873920021, w0=74.31840266943217, w1=16.653481079872765\n",
      "Stochastic Gradient Descent(46/49): loss=0.009252300048842046, w0=74.12506541953624, w1=16.276371401201487\n",
      "Stochastic Gradient Descent(47/49): loss=0.004494091745357329, w0=73.99032073262339, w1=16.297579082465766\n",
      "Stochastic Gradient Descent(48/49): loss=0.019611785434050406, w0=73.70883962641275, w1=16.626774601810745\n",
      "Stochastic Gradient Descent(49/49): loss=0.4829233197671835, w0=72.31205395500163, w1=16.654221831305055\n",
      "SGD: execution time=0.025 seconds\n"
     ]
    }
   ],
   "source": [
    "# from stochastic_gradient_descent import *\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.1\n",
    "# gamma = 0.7\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_losses, sgd_ws = stochastic_gradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8340a2b6967543abbc6113f20b84c04a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widgeâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        sgd_losses, sgd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gradient_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test implementation of SGD returning only last parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_2(\n",
    "        y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"Gradient descent algorithm.\"\"\"\n",
    "    # model's weights' vector\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # number of data points\n",
    "        N = len(y)\n",
    "        # compute stochastic gradient\n",
    "        stoch_gradient = compute_stoch_gradient(y, tx, w)\n",
    "        # compute error\n",
    "        e = compute_error_vec(y, tx, w)\n",
    "        # compute loss\n",
    "        loss = e.T @ e / (2 * N)\n",
    "        # update weights' vector\n",
    "        w = w - gamma * stoch_gradient\n",
    "\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65.93073010260338, array([74.06780585, 11.03489487]))"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descent_2(y, tx, initial_w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent_2(\n",
    "        y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"Stochastic gradient descent algorithm.\"\"\"\n",
    "    # model's weights' vector\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, batch_size):\n",
    "            # number of data points\n",
    "            N = len(minibatch_y)\n",
    "            # compute stochastic gradient\n",
    "            stoch_gradient = compute_stoch_gradient(minibatch_y, minibatch_tx, w)\n",
    "            # compute error\n",
    "            e = compute_error_vec(minibatch_y, minibatch_tx, w)\n",
    "            # compute loss\n",
    "            loss = e.T @ e / (2 * N)\n",
    "            # update weights' vector\n",
    "            w = w - gamma * stoch_gradient\n",
    "\n",
    "    return loss, w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14.005182406332784, array([75.88729955, 21.26056709]))"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stochastic_gradient_descent_2( y, tx, initial_w, batch_size, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from stochastic_gradient_descent import *\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 1000\n",
    "gamma = 0.01\n",
    "# gamma = 0.7\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "# w_initial = np.array([0, 0])\n",
    "initial_w = np.array([0, 0])\n",
    "\n",
    "# # Start SGD.\n",
    "# start_time = datetime.datetime.now()\n",
    "# sgd_losses, sgd_ws = stochastic_gradient_descent(\n",
    "#     y, tx, w_initial, batch_size, max_iters, gamma)\n",
    "# end_time = datetime.datetime.now()\n",
    "\n",
    "# # Print result\n",
    "# exection_time = (end_time - start_time).total_seconds()\n",
    "# print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.930052767335984, array([72.81023465, 13.43721703]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, w = stochastic_gradient_descent_2(y, tx, initial_w, batch_size, max_iters, gamma)\n",
    "loss, w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Effect of Outliers and MAE Cost Function, and Subgradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "\n",
    "Load and plot data containing outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "height, weight, gender = load_data(sub_sample=True, add_outlier=True)\n",
    "# height, weight, gender = load_data(sub_sample=True, add_outlier=False)\n",
    "# height, weight, gender = load_data(sub_sample=False, add_outlier=False)\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=2869.8351145358524, w0=7.4067805854926325, w1=1.103489486598918\n",
      "Gradient Descent(1/49): loss=2337.0932814935354, w0=14.072883112436008, w1=2.0966300245379443\n",
      "Gradient Descent(2/49): loss=1905.5723967292586, w0=20.07237538668505, w1=2.990456508683065\n",
      "Gradient Descent(3/49): loss=1556.0404800701936, w0=25.47191843350918, w1=3.7949003444136746\n",
      "Gradient Descent(4/49): loss=1272.9196275763516, w0=30.3315071756509, w1=4.5188997965712225\n",
      "Gradient Descent(5/49): loss=1043.5917370563393, w0=34.705137043578446, w1=5.170499303513016\n",
      "Gradient Descent(6/49): loss=857.8361457351293, w0=38.64140392471324, w1=5.756938859760629\n",
      "Gradient Descent(7/49): loss=707.3741167649493, w0=42.18404411773455, w1=6.284734460383481\n",
      "Gradient Descent(8/49): loss=585.4998732991036, w0=45.37242029145373, w1=6.7597505009440475\n",
      "Gradient Descent(9/49): loss=486.78173609176866, w0=48.241958847800994, w1=7.187264937448556\n",
      "Gradient Descent(10/49): loss=406.8200449538272, w0=50.82454354851353, w1=7.572027930302615\n",
      "Gradient Descent(11/49): loss=342.0510751320946, w0=53.148869779154815, w1=7.918314623871265\n",
      "Gradient Descent(12/49): loss=289.58820957649135, w0=55.24076338673197, w1=8.229972648083052\n",
      "Gradient Descent(13/49): loss=247.09328847645256, w0=57.12346763355141, w1=8.51046486987366\n",
      "Gradient Descent(14/49): loss=212.67240238542126, w0=58.817901455688904, w1=8.762907869485206\n",
      "Gradient Descent(15/49): loss=184.79148465168583, w0=60.34289189561265, w1=8.990106569135598\n",
      "Gradient Descent(16/49): loss=162.20794128736017, w0=61.71538329154402, w1=9.19458539882095\n",
      "Gradient Descent(17/49): loss=143.91527116225637, w0=62.950625547882254, w1=9.378616345537766\n",
      "Gradient Descent(18/49): loss=129.0982083609223, w0=64.06234357858666, w1=9.5442441975829\n",
      "Gradient Descent(19/49): loss=117.09638749184175, w0=65.06288980622064, w1=9.69330926442352\n",
      "Gradient Descent(20/49): loss=107.37491258788639, w0=65.9633814110912, w1=9.82746782458008\n",
      "Gradient Descent(21/49): loss=99.50051791568269, w0=66.77382385547472, w1=9.948210528720981\n",
      "Gradient Descent(22/49): loss=93.12225823119766, w0=67.50322205541988, w1=10.056878962447794\n",
      "Gradient Descent(23/49): loss=87.95586788676472, w0=68.15968043537053, w1=10.154680552801926\n",
      "Gradient Descent(24/49): loss=83.77109170777408, w0=68.75049297732612, w1=10.242701984120645\n",
      "Gradient Descent(25/49): loss=80.3814230027916, w0=69.28222426508614, w1=10.321921272307492\n",
      "Gradient Descent(26/49): loss=77.63579135175586, w0=69.76078242407017, w1=10.393218631675653\n",
      "Gradient Descent(27/49): loss=75.41182971441687, w0=70.19148476715579, w1=10.457386255106998\n",
      "Gradient Descent(28/49): loss=73.6104207881723, w0=70.57911687593284, w1=10.51513711619521\n",
      "Gradient Descent(29/49): loss=72.15127955791424, w0=70.92798577383219, w1=10.567112891174599\n",
      "Gradient Descent(30/49): loss=70.96937516140517, w0=71.24196778194161, w1=10.613891088656048\n",
      "Gradient Descent(31/49): loss=70.01203260023281, w0=71.52455158924009, w1=10.655991466389354\n",
      "Gradient Descent(32/49): loss=69.23658512568323, w0=71.77887701580872, w1=10.693881806349328\n",
      "Gradient Descent(33/49): loss=68.60847267129805, w0=72.00776989972049, w1=10.727983112313305\n",
      "Gradient Descent(34/49): loss=68.09970158324606, w0=72.21377349524107, w1=10.758674287680885\n",
      "Gradient Descent(35/49): loss=67.68759700192396, w0=72.39917673120961, w1=10.786296345511706\n",
      "Gradient Descent(36/49): loss=67.35379229105303, w0=72.56603964358128, w1=10.811156197559447\n",
      "Gradient Descent(37/49): loss=67.0834104752476, w0=72.71621626471578, w1=10.833530064402412\n",
      "Gradient Descent(38/49): loss=66.86440120444522, w0=72.85137522373684, w1=10.853666544561081\n",
      "Gradient Descent(39/49): loss=66.68700369509527, w0=72.97301828685579, w1=10.871789376703884\n",
      "Gradient Descent(40/49): loss=66.5433117125218, w0=73.08249704366285, w1=10.888099925632405\n",
      "Gradient Descent(41/49): loss=66.4269212066373, w0=73.1810279247892, w1=10.902779419668075\n",
      "Gradient Descent(42/49): loss=66.33264489687083, w0=73.26970571780292, w1=10.915990964300178\n",
      "Gradient Descent(43/49): loss=66.25628108596004, w0=73.34951573151527, w1=10.92788135446907\n",
      "Gradient Descent(44/49): loss=66.19442639912226, w0=73.42134474385638, w1=10.938582705621073\n",
      "Gradient Descent(45/49): loss=66.14432410278366, w0=73.48599085496338, w1=10.948213921657876\n",
      "Gradient Descent(46/49): loss=66.10374124274941, w0=73.54417235495967, w1=10.956882016090999\n",
      "Gradient Descent(47/49): loss=66.07086912612166, w0=73.59653570495634, w1=10.96468330108081\n",
      "Gradient Descent(48/49): loss=66.04424271165321, w0=73.64366271995334, w1=10.971704457571638\n",
      "Gradient Descent(49/49): loss=66.02267531593374, w0=73.68607703345064, w1=10.978023498413384\n",
      "Gradient Descent: execution time=0.012 seconds\n"
     ]
    }
   ],
   "source": [
    "# from gradient_descent import *\n",
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "# max_iters = 100\n",
    "# gamma = 0.7\n",
    "# gamma = 0.001\n",
    "gamma = 0.1\n",
    "# gamma = 0.1\n",
    "# gamma = 0.5\n",
    "# gamma = 1\n",
    "# gamma = 2\n",
    "# gamma = 2.5\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "# w_initial = np.array([100, 10])\n",
    "# w_initial = np.array([-1000, 1000])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "gradient_losses, gradient_ws = gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Gradient Descent: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widgeâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gradient_losses, gradient_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gradient_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Compare this figure to the two last ones. We can see that most of the points are less well described here than it is with the model using the MAE loss function.\n",
    "\n",
    "This highlights the fact that MSE is less robust to outliers than MAE loss function is!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the function `compute_loss(y, tx, w)` for the Mean Absolute Error cost function [here](#compute_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "height, weight, gender = load_data(sub_sample=True, add_outlier=True)\n",
    "# height, weight, gender = load_data(sub_sample=True, add_outlier=False)\n",
    "# height, weight, gender = load_data(sub_sample=False, add_outlier=False)\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search: loss*=1103.7693555530905, w0*=71.42857142857142, w1*=15.306122448979579, execution time=0.088 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from grid_search import generate_w, get_best_parameters\n",
    "from plots import grid_visualization\n",
    "\n",
    "# Generate the grid of parameters to be swept\n",
    "grid_w0, grid_w1 = generate_w(num_intervals=50)\n",
    "\n",
    "# Start the grid search\n",
    "start_time = datetime.datetime.now()\n",
    "grid_losses = grid_search(y, tx, grid_w0, grid_w1, method=\"MAE\")\n",
    "\n",
    "# Select the best combinaison\n",
    "loss_star, w0_star, w1_star = get_best_parameters(grid_w0, grid_w1, grid_losses)\n",
    "end_time = datetime.datetime.now()\n",
    "execution_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "# Print the results\n",
    "print(\"Grid Search: loss*={l}, w0*={w0}, w1*={w1}, execution time={t:.3f} seconds\".format(\n",
    "      l=loss_star, w0=w0_star, w1=w1_star, t=execution_time))\n",
    "\n",
    "# Plot the results\n",
    "fig = grid_visualization(grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight)\n",
    "fig.set_size_inches(10.0,6.0)\n",
    "fig.savefig(\"grid_plot\")  # Optional saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation of subgradient for MAE loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_subgradient(y, tx, w):\n",
    "    \"\"\"Compute the subgradient.\"\"\"\n",
    "    N = len(y)\n",
    "    e = compute_error_vec(y, tx, w)\n",
    "    return - tx.T @ np.sign(e) / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stochastic_subgradient(y, tx, w):\n",
    "    \"\"\"Compute the stochastic subgradient.\"\"\"\n",
    "    return compute_subgradient(y, tx, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subgradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"Subgradient descent algorithm.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    N = len(y)\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: compute subgradient and loss\n",
    "        # ***************************************************\n",
    "        subgradient = compute_subgradient(y, tx, w)\n",
    "        e = compute_error_vec(y, tx, w)\n",
    "        loss = compute_loss_MAE(y, tx, w)\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: update w by subgradient\n",
    "        # ***************************************************\n",
    "        # w -= gamma * subgradient\n",
    "        w = w - gamma * subgradient\n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\"Subgradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "        # non_diff_points_bool is a boolean vector, true in element i if e[i] is close to 0\n",
    "        non_diff_points_bool = np.isclose(e, np.zeros(len(e)))\n",
    "        # if any element in e is close to zero:\n",
    "        if any(non_diff_points_bool):\n",
    "            for p in np.where(non_diff_points_ind == True):\n",
    "                print(\"Non-differentiable point encountered at w_i, i = \" + p)\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_subgradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"Stochastic subgradient descent algorithm.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    N = len(y)\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, batch_size=1):\n",
    "            # ***************************************************\n",
    "            # INSERT YOUR CODE HERE\n",
    "            # TODO: compute subgradient and loss\n",
    "            # ***************************************************\n",
    "            subgradient = compute_stochastic_subgradient(y, tx, w)\n",
    "            e = compute_error_vec(y, tx, w)\n",
    "            loss = compute_loss_MAE(y, tx, w)\n",
    "            # ***************************************************\n",
    "            # INSERT YOUR CODE HERE\n",
    "            # TODO: update w by subgradient\n",
    "            # ***************************************************\n",
    "            # w -= gamma * subgradient\n",
    "            w = w - gamma * subgradient\n",
    "            # store w and loss\n",
    "            ws.append(w)\n",
    "            losses.append(loss)\n",
    "            print(\"Subgradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                  bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "            # non_diff_points_bool is a boolean vector, true in element i if e[i] is close to 0\n",
    "            non_diff_points_bool = np.isclose(e, np.zeros(len(e)))\n",
    "            # if any element in e is close to zero:\n",
    "            if any(non_diff_points_bool):\n",
    "                for p in np.where(non_diff_points_ind == True):\n",
    "                    print(\"Non-differentiable point encountered at w_i, i = \" + p)\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GD using MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subgradient Descent(0/199): loss=14961.69678269513, w0=0.7, w1=6.109524327590712e-16\n",
      "Subgradient Descent(1/199): loss=14820.296782695132, w0=1.4, w1=1.2219048655181425e-15\n",
      "Subgradient Descent(2/199): loss=14678.89678269513, w0=2.0999999999999996, w1=1.832857298277214e-15\n",
      "Subgradient Descent(3/199): loss=14537.496782695129, w0=2.8, w1=2.443809731036285e-15\n",
      "Subgradient Descent(4/199): loss=14396.096782695127, w0=3.5, w1=3.054762163795356e-15\n",
      "Subgradient Descent(5/199): loss=14254.69678269513, w0=4.2, w1=3.665714596554428e-15\n",
      "Subgradient Descent(6/199): loss=14113.296782695132, w0=4.9, w1=4.276667029313499e-15\n",
      "Subgradient Descent(7/199): loss=13971.896782695128, w0=5.6000000000000005, w1=4.887619462072571e-15\n",
      "Subgradient Descent(8/199): loss=13830.496782695127, w0=6.300000000000001, w1=5.498571894831642e-15\n",
      "Subgradient Descent(9/199): loss=13689.096782695127, w0=7.000000000000001, w1=6.109524327590714e-15\n",
      "Subgradient Descent(10/199): loss=13547.69678269513, w0=7.700000000000001, w1=6.720476760349785e-15\n",
      "Subgradient Descent(11/199): loss=13406.296782695128, w0=8.4, w1=7.331429193108857e-15\n",
      "Subgradient Descent(12/199): loss=13264.896782695128, w0=9.1, w1=7.942381625867928e-15\n",
      "Subgradient Descent(13/199): loss=13123.496782695125, w0=9.799999999999999, w1=8.553334058627e-15\n",
      "Subgradient Descent(14/199): loss=12982.096782695127, w0=10.499999999999998, w1=9.164286491386072e-15\n",
      "Subgradient Descent(15/199): loss=12840.69678269513, w0=11.199999999999998, w1=9.775238924145143e-15\n",
      "Subgradient Descent(16/199): loss=12699.296782695128, w0=11.899999999999997, w1=1.0386191356904215e-14\n",
      "Subgradient Descent(17/199): loss=12557.896782695128, w0=12.599999999999996, w1=1.0997143789663286e-14\n",
      "Subgradient Descent(18/199): loss=12416.496782695127, w0=13.299999999999995, w1=1.1608096222422358e-14\n",
      "Subgradient Descent(19/199): loss=12275.096782695131, w0=13.999999999999995, w1=1.2219048655181429e-14\n",
      "Subgradient Descent(20/199): loss=12133.69678269513, w0=14.699999999999994, w1=1.28300010879405e-14\n",
      "Subgradient Descent(21/199): loss=11992.296782695132, w0=15.399999999999993, w1=1.3440953520699572e-14\n",
      "Subgradient Descent(22/199): loss=11850.89678269513, w0=16.099999999999994, w1=1.4051905953458644e-14\n",
      "Subgradient Descent(23/199): loss=11709.496782695129, w0=16.799999999999994, w1=1.4662858386217714e-14\n",
      "Subgradient Descent(24/199): loss=11568.096782695131, w0=17.499999999999993, w1=1.5273810818976784e-14\n",
      "Subgradient Descent(25/199): loss=11426.69678269513, w0=18.199999999999992, w1=1.5884763251735854e-14\n",
      "Subgradient Descent(26/199): loss=11285.296782695132, w0=18.89999999999999, w1=1.6495715684494924e-14\n",
      "Subgradient Descent(27/199): loss=11143.89678269513, w0=19.59999999999999, w1=1.7106668117253994e-14\n",
      "Subgradient Descent(28/199): loss=11002.496782695129, w0=20.29999999999999, w1=1.7717620550013064e-14\n",
      "Subgradient Descent(29/199): loss=10861.096782695131, w0=20.99999999999999, w1=1.8328572982772134e-14\n",
      "Subgradient Descent(30/199): loss=10719.696782695131, w0=21.69999999999999, w1=1.8939525415531204e-14\n",
      "Subgradient Descent(31/199): loss=10578.296782695132, w0=22.399999999999988, w1=1.9550477848290273e-14\n",
      "Subgradient Descent(32/199): loss=10436.89678269513, w0=23.099999999999987, w1=2.0161430281049343e-14\n",
      "Subgradient Descent(33/199): loss=10295.496782695129, w0=23.799999999999986, w1=2.0772382713808413e-14\n",
      "Subgradient Descent(34/199): loss=10154.096782695131, w0=24.499999999999986, w1=2.1383335146567483e-14\n",
      "Subgradient Descent(35/199): loss=10012.696782695133, w0=25.199999999999985, w1=2.1994287579326553e-14\n",
      "Subgradient Descent(36/199): loss=9871.296782695132, w0=25.899999999999984, w1=2.2605240012085623e-14\n",
      "Subgradient Descent(37/199): loss=9729.89678269513, w0=26.599999999999984, w1=2.3216192444844693e-14\n",
      "Subgradient Descent(38/199): loss=9588.496782695129, w0=27.299999999999983, w1=2.3827144877603763e-14\n",
      "Subgradient Descent(39/199): loss=9447.096782695133, w0=27.999999999999982, w1=2.4438097310362833e-14\n",
      "Subgradient Descent(40/199): loss=9305.696782695133, w0=28.69999999999998, w1=2.5049049743121903e-14\n",
      "Subgradient Descent(41/199): loss=9164.296782695132, w0=29.39999999999998, w1=2.5660002175880973e-14\n",
      "Subgradient Descent(42/199): loss=9022.896782695132, w0=30.09999999999998, w1=2.6270954608640043e-14\n",
      "Subgradient Descent(43/199): loss=8881.496782695132, w0=30.79999999999998, w1=2.6881907041399113e-14\n",
      "Subgradient Descent(44/199): loss=8740.096782695135, w0=31.49999999999998, w1=2.7492859474158183e-14\n",
      "Subgradient Descent(45/199): loss=8598.696782695133, w0=32.19999999999998, w1=2.8103811906917253e-14\n",
      "Subgradient Descent(46/199): loss=8457.296782695132, w0=32.899999999999984, w1=2.871476433967632e-14\n",
      "Subgradient Descent(47/199): loss=8315.896782695132, w0=33.59999999999999, w1=2.9325716772435396e-14\n",
      "Subgradient Descent(48/199): loss=8174.496782695129, w0=34.29999999999999, w1=2.993666920519447e-14\n",
      "Subgradient Descent(49/199): loss=8033.09678269513, w0=34.99999999999999, w1=3.054762163795354e-14\n",
      "Subgradient Descent(50/199): loss=7891.6967826951295, w0=35.699999999999996, w1=3.1158574070712615e-14\n",
      "Subgradient Descent(51/199): loss=7750.296782695128, w0=36.4, w1=3.176952650347169e-14\n",
      "Subgradient Descent(52/199): loss=7608.89678269513, w0=37.1, w1=3.238047893623076e-14\n",
      "Subgradient Descent(53/199): loss=7467.496782695128, w0=37.800000000000004, w1=3.2991431368989835e-14\n",
      "Subgradient Descent(54/199): loss=7326.096782695127, w0=38.50000000000001, w1=3.360238380174891e-14\n",
      "Subgradient Descent(55/199): loss=7184.696782695128, w0=39.20000000000001, w1=3.421333623450798e-14\n",
      "Subgradient Descent(56/199): loss=7043.296782695126, w0=39.90000000000001, w1=3.4824288667267054e-14\n",
      "Subgradient Descent(57/199): loss=6901.896782695127, w0=40.600000000000016, w1=3.543524110002613e-14\n",
      "Subgradient Descent(58/199): loss=6760.496782695125, w0=41.30000000000002, w1=3.60461935327852e-14\n",
      "Subgradient Descent(59/199): loss=6619.096782695124, w0=42.00000000000002, w1=3.6657145965544273e-14\n",
      "Subgradient Descent(60/199): loss=6477.696782695124, w0=42.700000000000024, w1=3.7268098398303347e-14\n",
      "Subgradient Descent(61/199): loss=6336.296782695123, w0=43.40000000000003, w1=3.787905083106242e-14\n",
      "Subgradient Descent(62/199): loss=6194.896782695123, w0=44.10000000000003, w1=3.849000326382149e-14\n",
      "Subgradient Descent(63/199): loss=6053.496782695123, w0=44.80000000000003, w1=3.9100955696580566e-14\n",
      "Subgradient Descent(64/199): loss=5912.096782695121, w0=45.500000000000036, w1=3.971190812933964e-14\n",
      "Subgradient Descent(65/199): loss=5770.69678269512, w0=46.20000000000004, w1=4.032286056209871e-14\n",
      "Subgradient Descent(66/199): loss=5629.296782695121, w0=46.90000000000004, w1=4.0933812994857785e-14\n",
      "Subgradient Descent(67/199): loss=5489.000582353121, w0=47.59306930693074, w1=0.011147845678271063\n",
      "Subgradient Descent(68/199): loss=5351.071215877742, w0=48.279207920792125, w1=0.03308574108989941\n",
      "Subgradient Descent(69/199): loss=5215.076889199575, w0=48.96534653465351, w1=0.05502363650152776\n",
      "Subgradient Descent(70/199): loss=5081.317965800603, w0=49.63069306930698, w1=0.10538326388307814\n",
      "Subgradient Descent(71/199): loss=4953.868889606745, w0=50.28910891089114, w1=0.16746568532793435\n",
      "Subgradient Descent(72/199): loss=4827.65765989919, w0=50.947524752475296, w1=0.22954810677279056\n",
      "Subgradient Descent(73/199): loss=4703.447370982743, w0=51.59207920792084, w1=0.31242512932747524\n",
      "Subgradient Descent(74/199): loss=4582.749041724734, w0=52.22277227722777, w1=0.4119501328839991\n",
      "Subgradient Descent(75/199): loss=4465.466049067391, w0=52.84653465346539, w1=0.5208167847923756\n",
      "Subgradient Descent(76/199): loss=4350.639403257703, w0=53.4564356435644, w1=0.6457900912635992\n",
      "Subgradient Descent(77/199): loss=4239.24065467495, w0=54.0594059405941, w1=0.7796904498577214\n",
      "Subgradient Descent(78/199): loss=4130.00330919733, w0=54.655445544554496, w1=0.9197570104995693\n",
      "Subgradient Descent(79/199): loss=4022.205852109486, w0=55.24455445544559, w1=1.0670920297849913\n",
      "Subgradient Descent(80/199): loss=3916.7081062937723, w0=55.819801980198065, w1=1.2261255948210765\n",
      "Subgradient Descent(81/199): loss=3815.373791007969, w0=56.36732673267331, w1=1.410709342622213\n",
      "Subgradient Descent(82/199): loss=3720.024021374555, w0=56.900990099009945, w1=1.605853732220269\n",
      "Subgradient Descent(83/199): loss=3626.889505694158, w0=57.42772277227727, w1=1.808762802293962\n",
      "Subgradient Descent(84/199): loss=3536.1630466291244, w0=57.933663366336674, w1=2.0285064197514697\n",
      "Subgradient Descent(85/199): loss=3449.141400240186, w0=58.43267326732677, w1=2.2494370848672776\n",
      "Subgradient Descent(86/199): loss=3363.8993940970004, w0=58.91089108910895, w1=2.4837982986028337\n",
      "Subgradient Descent(87/199): loss=3282.205227762339, w0=59.382178217821824, w1=2.7260245553531504\n",
      "Subgradient Descent(88/199): loss=3201.51925295614, w0=59.83960396039608, w1=2.978742333469136\n",
      "Subgradient Descent(89/199): loss=3124.3177978287285, w0=60.262376237623805, w1=3.251528669355438\n",
      "Subgradient Descent(90/199): loss=3051.8755135454676, w0=60.67821782178222, w1=3.5270865794242794\n",
      "Subgradient Descent(91/199): loss=2980.489061876412, w0=61.087128712871326, w1=3.806459183951815\n",
      "Subgradient Descent(92/199): loss=2909.7148502472965, w0=61.49603960396043, w1=4.085831788479351\n",
      "Subgradient Descent(93/199): loss=2839.2689796817103, w0=61.891089108910926, w1=4.373839384328607\n",
      "Subgradient Descent(94/199): loss=2770.3534241443385, w0=62.27920792079211, w1=4.666037469532047\n",
      "Subgradient Descent(95/199): loss=2703.009734071399, w0=62.65346534653469, w1=4.959829093241769\n",
      "Subgradient Descent(96/199): loss=2637.8819662635797, w0=63.02079207920796, w1=5.25705719205664\n",
      "Subgradient Descent(97/199): loss=2573.5308483165263, w0=63.38118811881192, w1=5.560434316352406\n",
      "Subgradient Descent(98/199): loss=2509.4902155287346, w0=63.74158415841588, w1=5.863811440648173\n",
      "Subgradient Descent(99/199): loss=2445.7274698440365, w0=64.08811881188123, w1=6.172402175278548\n",
      "Subgradient Descent(100/199): loss=2383.725663674424, w0=64.42772277227726, w1=6.486369310516498\n",
      "Subgradient Descent(101/199): loss=2321.998442518578, w0=64.7673267326733, w1=6.800336445754448\n",
      "Subgradient Descent(102/199): loss=2260.2712213627324, w0=65.10693069306933, w1=7.114303580992399\n",
      "Subgradient Descent(103/199): loss=2198.5440002068867, w0=65.44653465346536, w1=7.428270716230349\n",
      "Subgradient Descent(104/199): loss=2138.087868479267, w0=65.76534653465349, w1=7.747893210218626\n",
      "Subgradient Descent(105/199): loss=2079.754939932426, w0=66.070297029703, w1=8.073669686866905\n",
      "Subgradient Descent(106/199): loss=2022.2931204067152, w0=66.37524752475251, w1=8.399446163515185\n",
      "Subgradient Descent(107/199): loss=1965.0730339869624, w0=66.6663366336634, w1=8.73297028041739\n",
      "Subgradient Descent(108/199): loss=1908.5213431467469, w0=66.9574257425743, w1=9.066494397319596\n",
      "Subgradient Descent(109/199): loss=1852.5503030275274, w0=67.23465346534658, w1=9.39863031947029\n",
      "Subgradient Descent(110/199): loss=1798.5385384941105, w0=67.51188118811886, w1=9.730766241620982\n",
      "Subgradient Descent(111/199): loss=1744.5267739606936, w0=67.78910891089114, w1=10.062902163771675\n",
      "Subgradient Descent(112/199): loss=1691.9826879010798, w0=68.06633663366343, w1=10.363999289979422\n",
      "Subgradient Descent(113/199): loss=1644.3892494278025, w0=68.32970297029709, w1=10.660466909273612\n",
      "Subgradient Descent(114/199): loss=1599.5459893226493, w0=68.59306930693076, w1=10.943174379960814\n",
      "Subgradient Descent(115/199): loss=1556.466505132154, w0=68.85643564356442, w1=11.225881850648015\n",
      "Subgradient Descent(116/199): loss=1513.7265578980853, w0=69.11287128712878, w1=11.504395843582206\n",
      "Subgradient Descent(117/199): loss=1472.5784659601702, w0=69.35544554455453, w1=11.78820189306775\n",
      "Subgradient Descent(118/199): loss=1433.6412752278718, w0=69.58415841584166, w1=12.060911465190971\n",
      "Subgradient Descent(119/199): loss=1397.8208695231226, w0=69.80594059405948, w1=12.324245668386048\n",
      "Subgradient Descent(120/199): loss=1363.6158525177216, w0=70.0277227722773, w1=12.587579871581125\n",
      "Subgradient Descent(121/199): loss=1330.1184517827442, w0=70.25643564356443, w1=12.824765405096484\n",
      "Subgradient Descent(122/199): loss=1298.9293418222567, w0=70.47821782178225, w1=13.065616959310148\n",
      "Subgradient Descent(123/199): loss=1268.1704393418513, w0=70.69306930693077, w1=13.302953389983912\n",
      "Subgradient Descent(124/199): loss=1238.9999925111913, w0=70.89405940594067, w1=13.525403099312918\n",
      "Subgradient Descent(125/199): loss=1213.179841265292, w0=71.08811881188126, w1=13.742945617944212\n",
      "Subgradient Descent(126/199): loss=1188.7744086950902, w0=71.27524752475254, w1=13.953548196006844\n",
      "Subgradient Descent(127/199): loss=1165.8703209584712, w0=71.46237623762383, w1=14.164150774069476\n",
      "Subgradient Descent(128/199): loss=1144.7667364816316, w0=71.62178217821788, w1=14.349779559473173\n",
      "Subgradient Descent(129/199): loss=1128.5188067306153, w0=71.75346534653471, w1=14.51689010761231\n",
      "Subgradient Descent(130/199): loss=1115.8172580563983, w0=71.87128712871292, w1=14.670791185324186\n",
      "Subgradient Descent(131/199): loss=1106.9789291355582, w0=71.95445544554461, w1=14.780276456654521\n",
      "Subgradient Descent(132/199): loss=1101.523776707409, w0=72.0376237623763, w1=14.889761727984856\n",
      "Subgradient Descent(133/199): loss=1096.333311434307, w0=72.10693069306937, w1=14.985916181776727\n",
      "Subgradient Descent(134/199): loss=1092.279134027916, w0=72.17623762376245, w1=15.082070635568597\n",
      "Subgradient Descent(135/199): loss=1088.2249566215253, w0=72.24554455445552, w1=15.178225089360467\n",
      "Subgradient Descent(136/199): loss=1084.8330776284165, w0=72.30099009900998, w1=15.25972348971591\n",
      "Subgradient Descent(137/199): loss=1082.1961177136175, w0=72.34950495049513, w1=15.335091856448138\n",
      "Subgradient Descent(138/199): loss=1079.8777113325618, w0=72.39801980198028, w1=15.410460223180365\n",
      "Subgradient Descent(139/199): loss=1077.8143612225294, w0=72.43267326732682, w1=15.469961786755725\n",
      "Subgradient Descent(140/199): loss=1076.668869914003, w0=72.46039603960405, w1=15.51864528583281\n",
      "Subgradient Descent(141/199): loss=1075.7866385111915, w0=72.48811881188128, w1=15.561592159086487\n",
      "Subgradient Descent(142/199): loss=1075.0796987583713, w0=72.5019801980199, w1=15.597828332032526\n",
      "Subgradient Descent(143/199): loss=1074.662484547909, w0=72.52277227722782, w1=15.624722856626713\n",
      "Subgradient Descent(144/199): loss=1074.3326135494844, w0=72.55049504950505, w1=15.642690329098\n",
      "Subgradient Descent(145/199): loss=1074.0824898101596, w0=72.56435643564366, w1=15.664356578291091\n",
      "Subgradient Descent(146/199): loss=1073.914122686212, w0=72.58514851485158, w1=15.677095775361284\n",
      "Subgradient Descent(147/199): loss=1073.742538778561, w0=72.6059405940595, w1=15.689834972431477\n",
      "Subgradient Descent(148/199): loss=1073.5709548709096, w0=72.62673267326743, w1=15.70257416950167\n",
      "Subgradient Descent(149/199): loss=1073.4031299462777, w0=72.64059405940604, w1=15.72424041869476\n",
      "Subgradient Descent(150/199): loss=1073.2365538680194, w0=72.66138613861396, w1=15.736979615764954\n",
      "Subgradient Descent(151/199): loss=1073.1003234829263, w0=72.66831683168327, w1=15.74811029423128\n",
      "Subgradient Descent(152/199): loss=1073.0507104044586, w0=72.67524752475258, w1=15.759240972697606\n",
      "Subgradient Descent(153/199): loss=1073.001097325991, w0=72.68217821782189, w1=15.770371651163932\n",
      "Subgradient Descent(154/199): loss=1072.9600803518836, w0=72.68217821782189, w1=15.774323911906686\n",
      "Subgradient Descent(155/199): loss=1072.955572760847, w0=72.68217821782189, w1=15.77827617264944\n",
      "Subgradient Descent(156/199): loss=1072.9510651698104, w0=72.68217821782189, w1=15.782228433392193\n",
      "Subgradient Descent(157/199): loss=1072.9465575787735, w0=72.68217821782189, w1=15.786180694134947\n",
      "Subgradient Descent(158/199): loss=1072.942049987737, w0=72.68217821782189, w1=15.7901329548777\n",
      "Subgradient Descent(159/199): loss=1072.9375423967, w0=72.68217821782189, w1=15.794085215620454\n",
      "Subgradient Descent(160/199): loss=1072.9330348056635, w0=72.68217821782189, w1=15.798037476363207\n",
      "Subgradient Descent(161/199): loss=1072.9285272146267, w0=72.68217821782189, w1=15.801989737105961\n",
      "Subgradient Descent(162/199): loss=1072.92401962359, w0=72.68217821782189, w1=15.805941997848715\n",
      "Subgradient Descent(163/199): loss=1072.9195120325533, w0=72.68217821782189, w1=15.809894258591468\n",
      "Subgradient Descent(164/199): loss=1072.9150044415164, w0=72.68217821782189, w1=15.813846519334222\n",
      "Subgradient Descent(165/199): loss=1072.91049685048, w0=72.68217821782189, w1=15.817798780076975\n",
      "Subgradient Descent(166/199): loss=1072.905989259443, w0=72.68217821782189, w1=15.821751040819729\n",
      "Subgradient Descent(167/199): loss=1072.9014816684062, w0=72.68217821782189, w1=15.825703301562482\n",
      "Subgradient Descent(168/199): loss=1072.8969740773698, w0=72.68217821782189, w1=15.829655562305236\n",
      "Subgradient Descent(169/199): loss=1072.892466486333, w0=72.68217821782189, w1=15.83360782304799\n",
      "Subgradient Descent(170/199): loss=1072.8879588952964, w0=72.68217821782189, w1=15.837560083790743\n",
      "Subgradient Descent(171/199): loss=1072.8834513042598, w0=72.68217821782189, w1=15.841512344533497\n",
      "Subgradient Descent(172/199): loss=1072.8789437132227, w0=72.68217821782189, w1=15.84546460527625\n",
      "Subgradient Descent(173/199): loss=1072.8744361221861, w0=72.68217821782189, w1=15.849416866019004\n",
      "Subgradient Descent(174/199): loss=1072.8699285311493, w0=72.68217821782189, w1=15.853369126761757\n",
      "Subgradient Descent(175/199): loss=1072.865420940113, w0=72.68217821782189, w1=15.857321387504511\n",
      "Subgradient Descent(176/199): loss=1072.8609133490759, w0=72.68217821782189, w1=15.861273648247264\n",
      "Subgradient Descent(177/199): loss=1072.8564057580393, w0=72.68217821782189, w1=15.865225908990018\n",
      "Subgradient Descent(178/199): loss=1072.8518981670027, w0=72.68217821782189, w1=15.869178169732772\n",
      "Subgradient Descent(179/199): loss=1072.8473905759658, w0=72.68217821782189, w1=15.873130430475525\n",
      "Subgradient Descent(180/199): loss=1072.842882984929, w0=72.68217821782189, w1=15.877082691218279\n",
      "Subgradient Descent(181/199): loss=1072.8383753938924, w0=72.68217821782189, w1=15.881034951961032\n",
      "Subgradient Descent(182/199): loss=1072.8338678028556, w0=72.68217821782189, w1=15.884987212703786\n",
      "Subgradient Descent(183/199): loss=1072.829360211819, w0=72.68217821782189, w1=15.88893947344654\n",
      "Subgradient Descent(184/199): loss=1072.8248526207824, w0=72.68217821782189, w1=15.892891734189293\n",
      "Subgradient Descent(185/199): loss=1072.8203450297458, w0=72.68217821782189, w1=15.896843994932047\n",
      "Subgradient Descent(186/199): loss=1072.8158374387087, w0=72.68217821782189, w1=15.9007962556748\n",
      "Subgradient Descent(187/199): loss=1072.8113298476724, w0=72.68217821782189, w1=15.904748516417554\n",
      "Subgradient Descent(188/199): loss=1072.8068222566353, w0=72.68217821782189, w1=15.908700777160307\n",
      "Subgradient Descent(189/199): loss=1072.8023146655987, w0=72.68217821782189, w1=15.91265303790306\n",
      "Subgradient Descent(190/199): loss=1072.804568624399, w0=72.67524752475258, w1=15.910526938117323\n",
      "Subgradient Descent(191/199): loss=1072.8002319116263, w0=72.67524752475258, w1=15.914479198860077\n",
      "Subgradient Descent(192/199): loss=1072.7957243205897, w0=72.67524752475258, w1=15.91843145960283\n",
      "Subgradient Descent(193/199): loss=1072.7942524828745, w0=72.66831683168327, w1=15.916305359817093\n",
      "Subgradient Descent(194/199): loss=1072.7936415666175, w0=72.66831683168327, w1=15.920257620559847\n",
      "Subgradient Descent(195/199): loss=1072.7891339755806, w0=72.66831683168327, w1=15.9242098813026\n",
      "Subgradient Descent(196/199): loss=1072.784626384544, w0=72.66831683168327, w1=15.928162142045354\n",
      "Subgradient Descent(197/199): loss=1072.7863611784142, w0=72.66138613861396, w1=15.926036042259616\n",
      "Subgradient Descent(198/199): loss=1072.7825436305716, w0=72.66138613861396, w1=15.92998830300237\n",
      "Subgradient Descent(199/199): loss=1072.778036039535, w0=72.66138613861396, w1=15.933940563745123\n",
      "Subgradient Descent: execution time=0.105 seconds\n"
     ]
    }
   ],
   "source": [
    "# from gradient_descent import *\n",
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 200\n",
    "# max_iters = 100\n",
    "# gamma = 0.7\n",
    "# gamma = 0.001\n",
    "gamma = 0.7\n",
    "# gamma = 0.1\n",
    "# gamma = 0.5\n",
    "# gamma = 1\n",
    "# gamma = 2\n",
    "# gamma = 2.5\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "# w_initial = np.array([100, 10])\n",
    "# w_initial = np.array([-1000, 1000])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "subgradient_losses, subgradient_ws = subgradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Subgradient Descent: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=201, min=1), Output()), _dom_classes=('widgâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        subgradient_losses, subgradient_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(subgradient_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SGD using MAE loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subgradient Descent(0/199): loss=14961.69678269513, w0=0.7, w1=6.109524327590712e-16\n",
      "Subgradient Descent(1/199): loss=14820.296782695132, w0=1.4, w1=1.2219048655181425e-15\n",
      "Subgradient Descent(2/199): loss=14678.89678269513, w0=2.0999999999999996, w1=1.832857298277214e-15\n",
      "Subgradient Descent(3/199): loss=14537.496782695129, w0=2.8, w1=2.443809731036285e-15\n",
      "Subgradient Descent(4/199): loss=14396.096782695127, w0=3.5, w1=3.054762163795356e-15\n",
      "Subgradient Descent(5/199): loss=14254.69678269513, w0=4.2, w1=3.665714596554428e-15\n",
      "Subgradient Descent(6/199): loss=14113.296782695132, w0=4.9, w1=4.276667029313499e-15\n",
      "Subgradient Descent(7/199): loss=13971.896782695128, w0=5.6000000000000005, w1=4.887619462072571e-15\n",
      "Subgradient Descent(8/199): loss=13830.496782695127, w0=6.300000000000001, w1=5.498571894831642e-15\n",
      "Subgradient Descent(9/199): loss=13689.096782695127, w0=7.000000000000001, w1=6.109524327590714e-15\n",
      "Subgradient Descent(10/199): loss=13547.69678269513, w0=7.700000000000001, w1=6.720476760349785e-15\n",
      "Subgradient Descent(11/199): loss=13406.296782695128, w0=8.4, w1=7.331429193108857e-15\n",
      "Subgradient Descent(12/199): loss=13264.896782695128, w0=9.1, w1=7.942381625867928e-15\n",
      "Subgradient Descent(13/199): loss=13123.496782695125, w0=9.799999999999999, w1=8.553334058627e-15\n",
      "Subgradient Descent(14/199): loss=12982.096782695127, w0=10.499999999999998, w1=9.164286491386072e-15\n",
      "Subgradient Descent(15/199): loss=12840.69678269513, w0=11.199999999999998, w1=9.775238924145143e-15\n",
      "Subgradient Descent(16/199): loss=12699.296782695128, w0=11.899999999999997, w1=1.0386191356904215e-14\n",
      "Subgradient Descent(17/199): loss=12557.896782695128, w0=12.599999999999996, w1=1.0997143789663286e-14\n",
      "Subgradient Descent(18/199): loss=12416.496782695127, w0=13.299999999999995, w1=1.1608096222422358e-14\n",
      "Subgradient Descent(19/199): loss=12275.096782695131, w0=13.999999999999995, w1=1.2219048655181429e-14\n",
      "Subgradient Descent(20/199): loss=12133.69678269513, w0=14.699999999999994, w1=1.28300010879405e-14\n",
      "Subgradient Descent(21/199): loss=11992.296782695132, w0=15.399999999999993, w1=1.3440953520699572e-14\n",
      "Subgradient Descent(22/199): loss=11850.89678269513, w0=16.099999999999994, w1=1.4051905953458644e-14\n",
      "Subgradient Descent(23/199): loss=11709.496782695129, w0=16.799999999999994, w1=1.4662858386217714e-14\n",
      "Subgradient Descent(24/199): loss=11568.096782695131, w0=17.499999999999993, w1=1.5273810818976784e-14\n",
      "Subgradient Descent(25/199): loss=11426.69678269513, w0=18.199999999999992, w1=1.5884763251735854e-14\n",
      "Subgradient Descent(26/199): loss=11285.296782695132, w0=18.89999999999999, w1=1.6495715684494924e-14\n",
      "Subgradient Descent(27/199): loss=11143.89678269513, w0=19.59999999999999, w1=1.7106668117253994e-14\n",
      "Subgradient Descent(28/199): loss=11002.496782695129, w0=20.29999999999999, w1=1.7717620550013064e-14\n",
      "Subgradient Descent(29/199): loss=10861.096782695131, w0=20.99999999999999, w1=1.8328572982772134e-14\n",
      "Subgradient Descent(30/199): loss=10719.696782695131, w0=21.69999999999999, w1=1.8939525415531204e-14\n",
      "Subgradient Descent(31/199): loss=10578.296782695132, w0=22.399999999999988, w1=1.9550477848290273e-14\n",
      "Subgradient Descent(32/199): loss=10436.89678269513, w0=23.099999999999987, w1=2.0161430281049343e-14\n",
      "Subgradient Descent(33/199): loss=10295.496782695129, w0=23.799999999999986, w1=2.0772382713808413e-14\n",
      "Subgradient Descent(34/199): loss=10154.096782695131, w0=24.499999999999986, w1=2.1383335146567483e-14\n",
      "Subgradient Descent(35/199): loss=10012.696782695133, w0=25.199999999999985, w1=2.1994287579326553e-14\n",
      "Subgradient Descent(36/199): loss=9871.296782695132, w0=25.899999999999984, w1=2.2605240012085623e-14\n",
      "Subgradient Descent(37/199): loss=9729.89678269513, w0=26.599999999999984, w1=2.3216192444844693e-14\n",
      "Subgradient Descent(38/199): loss=9588.496782695129, w0=27.299999999999983, w1=2.3827144877603763e-14\n",
      "Subgradient Descent(39/199): loss=9447.096782695133, w0=27.999999999999982, w1=2.4438097310362833e-14\n",
      "Subgradient Descent(40/199): loss=9305.696782695133, w0=28.69999999999998, w1=2.5049049743121903e-14\n",
      "Subgradient Descent(41/199): loss=9164.296782695132, w0=29.39999999999998, w1=2.5660002175880973e-14\n",
      "Subgradient Descent(42/199): loss=9022.896782695132, w0=30.09999999999998, w1=2.6270954608640043e-14\n",
      "Subgradient Descent(43/199): loss=8881.496782695132, w0=30.79999999999998, w1=2.6881907041399113e-14\n",
      "Subgradient Descent(44/199): loss=8740.096782695135, w0=31.49999999999998, w1=2.7492859474158183e-14\n",
      "Subgradient Descent(45/199): loss=8598.696782695133, w0=32.19999999999998, w1=2.8103811906917253e-14\n",
      "Subgradient Descent(46/199): loss=8457.296782695132, w0=32.899999999999984, w1=2.871476433967632e-14\n",
      "Subgradient Descent(47/199): loss=8315.896782695132, w0=33.59999999999999, w1=2.9325716772435396e-14\n",
      "Subgradient Descent(48/199): loss=8174.496782695129, w0=34.29999999999999, w1=2.993666920519447e-14\n",
      "Subgradient Descent(49/199): loss=8033.09678269513, w0=34.99999999999999, w1=3.054762163795354e-14\n",
      "Subgradient Descent(50/199): loss=7891.6967826951295, w0=35.699999999999996, w1=3.1158574070712615e-14\n",
      "Subgradient Descent(51/199): loss=7750.296782695128, w0=36.4, w1=3.176952650347169e-14\n",
      "Subgradient Descent(52/199): loss=7608.89678269513, w0=37.1, w1=3.238047893623076e-14\n",
      "Subgradient Descent(53/199): loss=7467.496782695128, w0=37.800000000000004, w1=3.2991431368989835e-14\n",
      "Subgradient Descent(54/199): loss=7326.096782695127, w0=38.50000000000001, w1=3.360238380174891e-14\n",
      "Subgradient Descent(55/199): loss=7184.696782695128, w0=39.20000000000001, w1=3.421333623450798e-14\n",
      "Subgradient Descent(56/199): loss=7043.296782695126, w0=39.90000000000001, w1=3.4824288667267054e-14\n",
      "Subgradient Descent(57/199): loss=6901.896782695127, w0=40.600000000000016, w1=3.543524110002613e-14\n",
      "Subgradient Descent(58/199): loss=6760.496782695125, w0=41.30000000000002, w1=3.60461935327852e-14\n",
      "Subgradient Descent(59/199): loss=6619.096782695124, w0=42.00000000000002, w1=3.6657145965544273e-14\n",
      "Subgradient Descent(60/199): loss=6477.696782695124, w0=42.700000000000024, w1=3.7268098398303347e-14\n",
      "Subgradient Descent(61/199): loss=6336.296782695123, w0=43.40000000000003, w1=3.787905083106242e-14\n",
      "Subgradient Descent(62/199): loss=6194.896782695123, w0=44.10000000000003, w1=3.849000326382149e-14\n",
      "Subgradient Descent(63/199): loss=6053.496782695123, w0=44.80000000000003, w1=3.9100955696580566e-14\n",
      "Subgradient Descent(64/199): loss=5912.096782695121, w0=45.500000000000036, w1=3.971190812933964e-14\n",
      "Subgradient Descent(65/199): loss=5770.69678269512, w0=46.20000000000004, w1=4.032286056209871e-14\n",
      "Subgradient Descent(66/199): loss=5629.296782695121, w0=46.90000000000004, w1=4.0933812994857785e-14\n",
      "Subgradient Descent(67/199): loss=5489.000582353121, w0=47.59306930693074, w1=0.011147845678271063\n",
      "Subgradient Descent(68/199): loss=5351.071215877742, w0=48.279207920792125, w1=0.03308574108989941\n",
      "Subgradient Descent(69/199): loss=5215.076889199575, w0=48.96534653465351, w1=0.05502363650152776\n",
      "Subgradient Descent(70/199): loss=5081.317965800603, w0=49.63069306930698, w1=0.10538326388307814\n",
      "Subgradient Descent(71/199): loss=4953.868889606745, w0=50.28910891089114, w1=0.16746568532793435\n",
      "Subgradient Descent(72/199): loss=4827.65765989919, w0=50.947524752475296, w1=0.22954810677279056\n",
      "Subgradient Descent(73/199): loss=4703.447370982743, w0=51.59207920792084, w1=0.31242512932747524\n",
      "Subgradient Descent(74/199): loss=4582.749041724734, w0=52.22277227722777, w1=0.4119501328839991\n",
      "Subgradient Descent(75/199): loss=4465.466049067391, w0=52.84653465346539, w1=0.5208167847923756\n",
      "Subgradient Descent(76/199): loss=4350.639403257703, w0=53.4564356435644, w1=0.6457900912635992\n",
      "Subgradient Descent(77/199): loss=4239.24065467495, w0=54.0594059405941, w1=0.7796904498577214\n",
      "Subgradient Descent(78/199): loss=4130.00330919733, w0=54.655445544554496, w1=0.9197570104995693\n",
      "Subgradient Descent(79/199): loss=4022.205852109486, w0=55.24455445544559, w1=1.0670920297849913\n",
      "Subgradient Descent(80/199): loss=3916.7081062937723, w0=55.819801980198065, w1=1.2261255948210765\n",
      "Subgradient Descent(81/199): loss=3815.373791007969, w0=56.36732673267331, w1=1.410709342622213\n",
      "Subgradient Descent(82/199): loss=3720.024021374555, w0=56.900990099009945, w1=1.605853732220269\n",
      "Subgradient Descent(83/199): loss=3626.889505694158, w0=57.42772277227727, w1=1.808762802293962\n",
      "Subgradient Descent(84/199): loss=3536.1630466291244, w0=57.933663366336674, w1=2.0285064197514697\n",
      "Subgradient Descent(85/199): loss=3449.141400240186, w0=58.43267326732677, w1=2.2494370848672776\n",
      "Subgradient Descent(86/199): loss=3363.8993940970004, w0=58.91089108910895, w1=2.4837982986028337\n",
      "Subgradient Descent(87/199): loss=3282.205227762339, w0=59.382178217821824, w1=2.7260245553531504\n",
      "Subgradient Descent(88/199): loss=3201.51925295614, w0=59.83960396039608, w1=2.978742333469136\n",
      "Subgradient Descent(89/199): loss=3124.3177978287285, w0=60.262376237623805, w1=3.251528669355438\n",
      "Subgradient Descent(90/199): loss=3051.8755135454676, w0=60.67821782178222, w1=3.5270865794242794\n",
      "Subgradient Descent(91/199): loss=2980.489061876412, w0=61.087128712871326, w1=3.806459183951815\n",
      "Subgradient Descent(92/199): loss=2909.7148502472965, w0=61.49603960396043, w1=4.085831788479351\n",
      "Subgradient Descent(93/199): loss=2839.2689796817103, w0=61.891089108910926, w1=4.373839384328607\n",
      "Subgradient Descent(94/199): loss=2770.3534241443385, w0=62.27920792079211, w1=4.666037469532047\n",
      "Subgradient Descent(95/199): loss=2703.009734071399, w0=62.65346534653469, w1=4.959829093241769\n",
      "Subgradient Descent(96/199): loss=2637.8819662635797, w0=63.02079207920796, w1=5.25705719205664\n",
      "Subgradient Descent(97/199): loss=2573.5308483165263, w0=63.38118811881192, w1=5.560434316352406\n",
      "Subgradient Descent(98/199): loss=2509.4902155287346, w0=63.74158415841588, w1=5.863811440648173\n",
      "Subgradient Descent(99/199): loss=2445.7274698440365, w0=64.08811881188123, w1=6.172402175278548\n",
      "Subgradient Descent(100/199): loss=2383.725663674424, w0=64.42772277227726, w1=6.486369310516498\n",
      "Subgradient Descent(101/199): loss=2321.998442518578, w0=64.7673267326733, w1=6.800336445754448\n",
      "Subgradient Descent(102/199): loss=2260.2712213627324, w0=65.10693069306933, w1=7.114303580992399\n",
      "Subgradient Descent(103/199): loss=2198.5440002068867, w0=65.44653465346536, w1=7.428270716230349\n",
      "Subgradient Descent(104/199): loss=2138.087868479267, w0=65.76534653465349, w1=7.747893210218626\n",
      "Subgradient Descent(105/199): loss=2079.754939932426, w0=66.070297029703, w1=8.073669686866905\n",
      "Subgradient Descent(106/199): loss=2022.2931204067152, w0=66.37524752475251, w1=8.399446163515185\n",
      "Subgradient Descent(107/199): loss=1965.0730339869624, w0=66.6663366336634, w1=8.73297028041739\n",
      "Subgradient Descent(108/199): loss=1908.5213431467469, w0=66.9574257425743, w1=9.066494397319596\n",
      "Subgradient Descent(109/199): loss=1852.5503030275274, w0=67.23465346534658, w1=9.39863031947029\n",
      "Subgradient Descent(110/199): loss=1798.5385384941105, w0=67.51188118811886, w1=9.730766241620982\n",
      "Subgradient Descent(111/199): loss=1744.5267739606936, w0=67.78910891089114, w1=10.062902163771675\n",
      "Subgradient Descent(112/199): loss=1691.9826879010798, w0=68.06633663366343, w1=10.363999289979422\n",
      "Subgradient Descent(113/199): loss=1644.3892494278025, w0=68.32970297029709, w1=10.660466909273612\n",
      "Subgradient Descent(114/199): loss=1599.5459893226493, w0=68.59306930693076, w1=10.943174379960814\n",
      "Subgradient Descent(115/199): loss=1556.466505132154, w0=68.85643564356442, w1=11.225881850648015\n",
      "Subgradient Descent(116/199): loss=1513.7265578980853, w0=69.11287128712878, w1=11.504395843582206\n",
      "Subgradient Descent(117/199): loss=1472.5784659601702, w0=69.35544554455453, w1=11.78820189306775\n",
      "Subgradient Descent(118/199): loss=1433.6412752278718, w0=69.58415841584166, w1=12.060911465190971\n",
      "Subgradient Descent(119/199): loss=1397.8208695231226, w0=69.80594059405948, w1=12.324245668386048\n",
      "Subgradient Descent(120/199): loss=1363.6158525177216, w0=70.0277227722773, w1=12.587579871581125\n",
      "Subgradient Descent(121/199): loss=1330.1184517827442, w0=70.25643564356443, w1=12.824765405096484\n",
      "Subgradient Descent(122/199): loss=1298.9293418222567, w0=70.47821782178225, w1=13.065616959310148\n",
      "Subgradient Descent(123/199): loss=1268.1704393418513, w0=70.69306930693077, w1=13.302953389983912\n",
      "Subgradient Descent(124/199): loss=1238.9999925111913, w0=70.89405940594067, w1=13.525403099312918\n",
      "Subgradient Descent(125/199): loss=1213.179841265292, w0=71.08811881188126, w1=13.742945617944212\n",
      "Subgradient Descent(126/199): loss=1188.7744086950902, w0=71.27524752475254, w1=13.953548196006844\n",
      "Subgradient Descent(127/199): loss=1165.8703209584712, w0=71.46237623762383, w1=14.164150774069476\n",
      "Subgradient Descent(128/199): loss=1144.7667364816316, w0=71.62178217821788, w1=14.349779559473173\n",
      "Subgradient Descent(129/199): loss=1128.5188067306153, w0=71.75346534653471, w1=14.51689010761231\n",
      "Subgradient Descent(130/199): loss=1115.8172580563983, w0=71.87128712871292, w1=14.670791185324186\n",
      "Subgradient Descent(131/199): loss=1106.9789291355582, w0=71.95445544554461, w1=14.780276456654521\n",
      "Subgradient Descent(132/199): loss=1101.523776707409, w0=72.0376237623763, w1=14.889761727984856\n",
      "Subgradient Descent(133/199): loss=1096.333311434307, w0=72.10693069306937, w1=14.985916181776727\n",
      "Subgradient Descent(134/199): loss=1092.279134027916, w0=72.17623762376245, w1=15.082070635568597\n",
      "Subgradient Descent(135/199): loss=1088.2249566215253, w0=72.24554455445552, w1=15.178225089360467\n",
      "Subgradient Descent(136/199): loss=1084.8330776284165, w0=72.30099009900998, w1=15.25972348971591\n",
      "Subgradient Descent(137/199): loss=1082.1961177136175, w0=72.34950495049513, w1=15.335091856448138\n",
      "Subgradient Descent(138/199): loss=1079.8777113325618, w0=72.39801980198028, w1=15.410460223180365\n",
      "Subgradient Descent(139/199): loss=1077.8143612225294, w0=72.43267326732682, w1=15.469961786755725\n",
      "Subgradient Descent(140/199): loss=1076.668869914003, w0=72.46039603960405, w1=15.51864528583281\n",
      "Subgradient Descent(141/199): loss=1075.7866385111915, w0=72.48811881188128, w1=15.561592159086487\n",
      "Subgradient Descent(142/199): loss=1075.0796987583713, w0=72.5019801980199, w1=15.597828332032526\n",
      "Subgradient Descent(143/199): loss=1074.662484547909, w0=72.52277227722782, w1=15.624722856626713\n",
      "Subgradient Descent(144/199): loss=1074.3326135494844, w0=72.55049504950505, w1=15.642690329098\n",
      "Subgradient Descent(145/199): loss=1074.0824898101596, w0=72.56435643564366, w1=15.664356578291091\n",
      "Subgradient Descent(146/199): loss=1073.914122686212, w0=72.58514851485158, w1=15.677095775361284\n",
      "Subgradient Descent(147/199): loss=1073.742538778561, w0=72.6059405940595, w1=15.689834972431477\n",
      "Subgradient Descent(148/199): loss=1073.5709548709096, w0=72.62673267326743, w1=15.70257416950167\n",
      "Subgradient Descent(149/199): loss=1073.4031299462777, w0=72.64059405940604, w1=15.72424041869476\n",
      "Subgradient Descent(150/199): loss=1073.2365538680194, w0=72.66138613861396, w1=15.736979615764954\n",
      "Subgradient Descent(151/199): loss=1073.1003234829263, w0=72.66831683168327, w1=15.74811029423128\n",
      "Subgradient Descent(152/199): loss=1073.0507104044586, w0=72.67524752475258, w1=15.759240972697606\n",
      "Subgradient Descent(153/199): loss=1073.001097325991, w0=72.68217821782189, w1=15.770371651163932\n",
      "Subgradient Descent(154/199): loss=1072.9600803518836, w0=72.68217821782189, w1=15.774323911906686\n",
      "Subgradient Descent(155/199): loss=1072.955572760847, w0=72.68217821782189, w1=15.77827617264944\n",
      "Subgradient Descent(156/199): loss=1072.9510651698104, w0=72.68217821782189, w1=15.782228433392193\n",
      "Subgradient Descent(157/199): loss=1072.9465575787735, w0=72.68217821782189, w1=15.786180694134947\n",
      "Subgradient Descent(158/199): loss=1072.942049987737, w0=72.68217821782189, w1=15.7901329548777\n",
      "Subgradient Descent(159/199): loss=1072.9375423967, w0=72.68217821782189, w1=15.794085215620454\n",
      "Subgradient Descent(160/199): loss=1072.9330348056635, w0=72.68217821782189, w1=15.798037476363207\n",
      "Subgradient Descent(161/199): loss=1072.9285272146267, w0=72.68217821782189, w1=15.801989737105961\n",
      "Subgradient Descent(162/199): loss=1072.92401962359, w0=72.68217821782189, w1=15.805941997848715\n",
      "Subgradient Descent(163/199): loss=1072.9195120325533, w0=72.68217821782189, w1=15.809894258591468\n",
      "Subgradient Descent(164/199): loss=1072.9150044415164, w0=72.68217821782189, w1=15.813846519334222\n",
      "Subgradient Descent(165/199): loss=1072.91049685048, w0=72.68217821782189, w1=15.817798780076975\n",
      "Subgradient Descent(166/199): loss=1072.905989259443, w0=72.68217821782189, w1=15.821751040819729\n",
      "Subgradient Descent(167/199): loss=1072.9014816684062, w0=72.68217821782189, w1=15.825703301562482\n",
      "Subgradient Descent(168/199): loss=1072.8969740773698, w0=72.68217821782189, w1=15.829655562305236\n",
      "Subgradient Descent(169/199): loss=1072.892466486333, w0=72.68217821782189, w1=15.83360782304799\n",
      "Subgradient Descent(170/199): loss=1072.8879588952964, w0=72.68217821782189, w1=15.837560083790743\n",
      "Subgradient Descent(171/199): loss=1072.8834513042598, w0=72.68217821782189, w1=15.841512344533497\n",
      "Subgradient Descent(172/199): loss=1072.8789437132227, w0=72.68217821782189, w1=15.84546460527625\n",
      "Subgradient Descent(173/199): loss=1072.8744361221861, w0=72.68217821782189, w1=15.849416866019004\n",
      "Subgradient Descent(174/199): loss=1072.8699285311493, w0=72.68217821782189, w1=15.853369126761757\n",
      "Subgradient Descent(175/199): loss=1072.865420940113, w0=72.68217821782189, w1=15.857321387504511\n",
      "Subgradient Descent(176/199): loss=1072.8609133490759, w0=72.68217821782189, w1=15.861273648247264\n",
      "Subgradient Descent(177/199): loss=1072.8564057580393, w0=72.68217821782189, w1=15.865225908990018\n",
      "Subgradient Descent(178/199): loss=1072.8518981670027, w0=72.68217821782189, w1=15.869178169732772\n",
      "Subgradient Descent(179/199): loss=1072.8473905759658, w0=72.68217821782189, w1=15.873130430475525\n",
      "Subgradient Descent(180/199): loss=1072.842882984929, w0=72.68217821782189, w1=15.877082691218279\n",
      "Subgradient Descent(181/199): loss=1072.8383753938924, w0=72.68217821782189, w1=15.881034951961032\n",
      "Subgradient Descent(182/199): loss=1072.8338678028556, w0=72.68217821782189, w1=15.884987212703786\n",
      "Subgradient Descent(183/199): loss=1072.829360211819, w0=72.68217821782189, w1=15.88893947344654\n",
      "Subgradient Descent(184/199): loss=1072.8248526207824, w0=72.68217821782189, w1=15.892891734189293\n",
      "Subgradient Descent(185/199): loss=1072.8203450297458, w0=72.68217821782189, w1=15.896843994932047\n",
      "Subgradient Descent(186/199): loss=1072.8158374387087, w0=72.68217821782189, w1=15.9007962556748\n",
      "Subgradient Descent(187/199): loss=1072.8113298476724, w0=72.68217821782189, w1=15.904748516417554\n",
      "Subgradient Descent(188/199): loss=1072.8068222566353, w0=72.68217821782189, w1=15.908700777160307\n",
      "Subgradient Descent(189/199): loss=1072.8023146655987, w0=72.68217821782189, w1=15.91265303790306\n",
      "Subgradient Descent(190/199): loss=1072.804568624399, w0=72.67524752475258, w1=15.910526938117323\n",
      "Subgradient Descent(191/199): loss=1072.8002319116263, w0=72.67524752475258, w1=15.914479198860077\n",
      "Subgradient Descent(192/199): loss=1072.7957243205897, w0=72.67524752475258, w1=15.91843145960283\n",
      "Subgradient Descent(193/199): loss=1072.7942524828745, w0=72.66831683168327, w1=15.916305359817093\n",
      "Subgradient Descent(194/199): loss=1072.7936415666175, w0=72.66831683168327, w1=15.920257620559847\n",
      "Subgradient Descent(195/199): loss=1072.7891339755806, w0=72.66831683168327, w1=15.9242098813026\n",
      "Subgradient Descent(196/199): loss=1072.784626384544, w0=72.66831683168327, w1=15.928162142045354\n",
      "Subgradient Descent(197/199): loss=1072.7863611784142, w0=72.66138613861396, w1=15.926036042259616\n",
      "Subgradient Descent(198/199): loss=1072.7825436305716, w0=72.66138613861396, w1=15.92998830300237\n",
      "Subgradient Descent(199/199): loss=1072.778036039535, w0=72.66138613861396, w1=15.933940563745123\n",
      "Subgradient Descent: execution time=0.129 seconds\n"
     ]
    }
   ],
   "source": [
    "# from gradient_descent import *\n",
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 200\n",
    "# max_iters = 100\n",
    "# gamma = 0.7\n",
    "# gamma = 0.001\n",
    "gamma = 0.7\n",
    "# gamma = 0.1\n",
    "# gamma = 0.5\n",
    "# gamma = 1\n",
    "# gamma = 2\n",
    "# gamma = 2.5\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "# w_initial = np.array([100, 10])\n",
    "# w_initial = np.array([-1000, 1000])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "subgradient_losses, subgradient_ws = stochastic_subgradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Subgradient Descent: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc9e36c502e049fcb6524135e3eeb048",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=201, min=1), Output()), _dom_classes=('widgâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        subgradient_losses, subgradient_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(subgradient_ws)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
